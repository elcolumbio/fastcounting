{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import redis\n",
    "\n",
    "from fastcounting import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you have to run everything exactly once to not run in bugs :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = '2017-13'\n",
    "r = redis.Redis(**helper.Helper().rediscred, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main thing\n",
    "files = find_batch_files(month)\n",
    "\n",
    "dfinput = read_lexware_journal(files)\n",
    "df = dfinput.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_lexware_journal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_walk(df, files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nr.'].ffill(inplace=True) # this we have to do between first and second walk\n",
    "\n",
    "second_walk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_batch_files(month):\n",
    "    p = helper.Helper().datafolder(month)\n",
    "    files = [file for file in p.iterdir() if file.parts[-1].lower().startswith('journal')]\n",
    "    return files\n",
    "\n",
    "def read_lexware_journal(files, nrows=None):\n",
    "    \"\"\"Read xlxs from the default folder for each year e.g. month=2018-13 or actual month.\"\"\"\n",
    "    data = pd.read_excel(\n",
    "        files[0], bom=True, sep=';', encoding='latin-1', decimal=',', thousands='.',\n",
    "        dayfirst=True, skiprows=1, parse_dates=['Belegdat.', 'Buchdat.', 'Jour. Dat.'], nrows=nrows)\n",
    "    return data\n",
    "\n",
    "def clean_lexware_journal(df):\n",
    "    df = dfinput.copy()\n",
    "    for column in ['Belegdat.', 'Buchdat.', 'Jour. Dat.']:\n",
    "        df[column] = df[column].ffill() # this works cause data is sorted\n",
    "        df[column] = (df[column] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "        df[column] = df[column].apply(str)\n",
    "\n",
    "    # multiply all currency amounts to get integers\n",
    "    for money_column in ['SollEUR', 'HabenEUR', 'USt H-EUR', 'USt-S EUR']:\n",
    "        df[money_column] = df[money_column]*100\n",
    "        # we don't cast to integer because we have slightly meaningful nans\n",
    "        df[money_column] = df[money_column].round(0)\n",
    "\n",
    "    dimensions = ['Sollkto', 'Habenkto', 'USt Kto-H', 'USt Kto-S']\n",
    "    df.fillna(value={dimension: 0.0 for dimension in dimensions}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_walk(df, batchtext):\n",
    "    \"\"\"\n",
    "    First of two walks for splitted multirow transactions only the first row\n",
    "    has general information, we run only over those here.\n",
    "    \"\"\"\n",
    "    batchID = r.incr('next_batchID')  # rollback and diff functionality\n",
    "    r.hmset(f'batchID:{batchID}',\n",
    "        {'text': batchtext})\n",
    "    for i in df.index:\n",
    "        # get unique id from database (threadsave)\n",
    "        generalID = r.incr('next_generalID')\n",
    "        # create mapping for rollback if we only run first_walk\n",
    "        r.sadd(f'batch:general:{batchID}', generalID)\n",
    "        # create temporary mapping\n",
    "        r.set(df.at[i, 'Nr.'], generalID, ex=300)\n",
    "        # store data in hash\n",
    "        r.hmset(f'generalID:{generalID}',\n",
    "            {'date': df.at[i, 'Belegdat.'],\n",
    "            'jourdat': df.at[i, 'Jour. Dat.'],\n",
    "            'buchdat': df.at[i, 'Buchdat.'],\n",
    "            'status': df.at[i, 'Status'],\n",
    "            'belegnr': df.at[i, 'Belegnr.']})\n",
    "\n",
    "        \n",
    "def atomic_to_redis(i, konto, betrag, kontenseite, ust=None):\n",
    "    # get unique id from database (threadsave)\n",
    "    atomicID = r.incr('next_atomicID')\n",
    "    # get temporary mapping we created in the first walk\n",
    "    generalID = r.get(df.at[i, 'Nr.'])\n",
    "    batchID = r.get('next_batchID')\n",
    "    # create a lookup set for all atomis in a batch\n",
    "    r.sadd(f'batch:atomic:{batchID}', atomicID)\n",
    "    # create stable mapping- general:atomic:\n",
    "    r.sadd(f'general:atomic:{generalID}', atomicID)\n",
    "    # create mapping accountID:atomicID\n",
    "    r.sadd(f'account:atomic:{konto}', atomicID)\n",
    "    # create datefilter atomic:date\n",
    "    r.zadd('atomic:date', {atomicID: int(df.at[i, 'Belegdat.'])}) # could think about splitting the key into years\n",
    "    # store data in hash + mapping atomic:general + mapping atomic+account\n",
    "    r.hmset(f'atomicID:{atomicID}',\n",
    "           {'generalID': generalID,\n",
    "            'accountID': konto,\n",
    "            'text': df.at[i, 'Buchungstext'],\n",
    "            'amount': betrag,\n",
    "            'kontenseite': kontenseite,\n",
    "            'batchID': batchID})\n",
    "        \n",
    "def second_walk(df):\n",
    "    \"\"\"\n",
    "    Second and last walk, now we walk over every row and we unpack up to 4 dimensions per row.\n",
    "    There are 3 types of accounting transactions in this row based lexware export.\n",
    "    1. automatic transaction, like ust payment on revenues\n",
    "    2. split multirow transaction, like payment of import taxes and handling with dhl.\n",
    "        important to note you can make split multirow transactions with duplicated \n",
    "        accounts thats why we cant use a dictionary here.\n",
    "    3. standard account to account mapping\n",
    "    + every combination from the above\n",
    "    It comes handy that split multirow transaction are seperated in rows.\n",
    "    \"\"\"\n",
    "    for i in df.index:\n",
    "        if df.at[i, 'Sollkto']:\n",
    "            atomic_to_redis(i, df.at[i, 'Sollkto'], -df.at[i, 'SollEUR'], 'Soll')\n",
    "\n",
    "        if df.at[i, 'Habenkto']:\n",
    "            atomic_to_redis(i, df.at[i, 'Habenkto'], df.at[i, 'HabenEUR'], 'Haben')\n",
    "\n",
    "        if df.at[i, 'USt Kto-H']:\n",
    "            atomic_to_redis(i, df.at[i, 'USt Kto-H'], df.at[i, 'USt H-EUR'], 'Haben')\n",
    "\n",
    "        if df.at[i, 'USt Kto-S']:\n",
    "            atomic_to_redis(i, df.at[i, 'USt Kto-S'], -df.at[i, 'USt-S EUR'], 'Soll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
